# Reinforcement Learning

### 목차

**1. 강화학습 개요** </br>
  - [강화학습의 본질](#강화학습의-본질) </br>
  - [강화학습의 특징](#강화학습의-특징) </br>
  - [강화학습의 구성요소](#강화-학습의-구성-요소) </br>

**2. 고전적 강화학습** </br>
  - [고전적 강화학습 분류](#고전적-강화학습-분류) </br>
  - [마르코프 의사 결정](#마르코프-결정-과정) </br>
      - [Markov Processes Markov Chain](#markov-processes-markov-chain) </br>
      - [마르코프 보상 과정](#마르코프-보상-과정) </br>
        - [리턴](#리턴) </br>
        - [MRP의 가치 함수(Value Function)](#MRP의_가치-함수value-function) </br>
        - [Bellman 방정식](#Bellman_방정식) </br>
      - [마르코프 결정 과정 (MDP)의 태동](#마르코프-결정-과정-MDP의-태동) </br>
      - [마르코프 결정 과정](#마르코프-결정-과정) </br>
      - [정책함수(Policy Function)](#정책함수(policy-function)) </br>
      - [MDP, MRP, MP의 관계](#MDP-MRP-MP의-관계) </br>
        - [MDP(마르코프 결정 과정)의 가치함수](#mdp마르코프-결정-과정의-가치함수) </br>
  - [강화학습 문제 풀이 기법](#강화학습-문제-풀이-기법) </br>

---
## [강화학습 개요]
### [강화학습의 본질]
- 강화학습이 풀고 싶어하는 문제 : 주어진 상황에서 가장 좋은 행동을 하는 것 </br></br>


### [강화학습의 특징]
**1. 정답이 아닌 '보상'이 주어짐**
   - label, 정답이 주어지지 않음
   - 보상이 주어짐 (좋고, 나쁨을 평가)

**2. 현재 의사 결정이 미래의 의사결정에 영향을 미침**</br></br>
**3. 에이전트는 문제의 구조를 알 수 없음**
   - 환경과의 상호 작용을 통해 정보를 관측
   - 관측된 정보를 최대한 모아서 학습하게 됨 </br></br>


### [강화 학습의 구성 요소]
- **agent, 환경**
  - **agent** : '학습의 주체'로 환경과의 상호작용을 함
  - **환경** : 강화 학습 문제의 특성을 지님 </br></br>
    - 강화학습 agent는 현재와 미래의 보상을 모두 잘 고려해야 (개념 : 리턴) </br>
      ! 보상을 최대화한다고 해서 전체적 목적 성취에 도움이 되지 않을 수 있다. 

- **일반적인 상호작용의 과정**
  - 매 시점마다 agent는 환경으로부터 현재를 관측 : s_{t}
  - 관측된 내용을 통해 적합한 행동을 환경에 가함  : a_{t}
  - agent의 행동(a_{t})에 **영향을 받은 환경은 agent에** 다음을 전달</br>
</br></br>

**[환경과 agent의 구성요소]**
- **1. 환경** : agent에 보상(reward)와 상태(state)를 전달
  - **다음 상태** : s_{t+1}
  - **보상** : r_{r+1} </br></br>
    - **보상** : 하나의 숫자로 표현된 행동에 대한 평가지표
      - 현재 상태에 agent에 가해진 action이 얼마나 좋은지, 나쁜지에 대한 평가지표
      - 예시) 길찾기 문제에서 각 도시 사이의 거리 </br></br>
    - **상태** : 문제의 현재 상황을 잘 기술하는 정보
      - 예시) 길 찾기 문제에서 에이전트의 현 위치 </br></br>

- **2. agent의 구성요소**
  - 정책, 가치함수, 모델 </br>
  - **정책(policy)**
    - RL의 목적이 "좋은 정책을 찾는 것"과 밀접하므로 가장 중요
    - 특정 상황에서 agent가 어떻게 행동을 할지 결정해주는 함수
    - π(α | s)= P(A=α | S=s) </br>
      : 현재 우리에게 주어진 상황 s를 고려했을 때 행동 α를 고를 확률 
  - **가치 함수(value function)**
    - 환경이 agent에 보상을 전달해주지만, agent는 환경에 대한 선험적 지식이 없음
    - 가치함수를 통해 현재 상태 및 행동이 얼마나 좋은지 추산
  - **모델**
    - agent가 추측하는 세상의 형태
    - agent 내부에 존재하는 환경에 대한 추측치

---
### 목차
**1. 강화학습 개요** </br>
  - [강화학습의 본질](#강화학습의-본질) </br>
  - [강화학습의 특징](#강화학습의-특징) </br>
  - [강화학습의 구성요소](#강화-학습의-구성-요소) </br>

**2. 고전적 강화학습** </br>
  - [고전적 강화학습 분류](#고전적-강화학습-분류) </br>
  - [마르코프 의사 결정](#마르코프-결정-과정) </br>
      - [Markov Processes Markov Chain](#markov-processes-markov-chain) </br>
      - [마르코프 보상 과정](#마르코프-보상-과정) </br>
        - [리턴](#리턴) </br>
        - [가치 함수(Value Function)](#가치-함수value-function)
  - [강화학습 문제 풀이 기법](#강화학습-문제-풀이-기법) </br>
---
# [고전적 강화학습]

## [고전적 강화학습 분류]
- 환경에 대한 모델이 있는지 없는지에 따라 분류된다.</br>
  - 모델 기반 강화학습 </br>
  - 모델 없는 강화학습 </br>
    - 일반적으로 지칭되는 '강화학습' </br>
      (1) 가치 기반 강화학습 : 가치 함수를 직접적으로 추산하는 방법 </br>
      (2) 정책 기반 강화학습 : 가치 함수 대신 바로 정책 함수를 추산 </br>
      (3) Actor-Critic 알고리즘 </br>
        - 가치 기반 강화학습, 정책 기반 강화학습의 장점을 살린 새로운 계열 알고리즘 </br>
        
## [마르코프 결정 과정]
(Markov Decision Process: MDP)
- 강화학습 문제 해결 알고리즘 설계 위해 수학적 정리 필요 
- 많은 강화학습은 MDP(마르코프 의사 결정 과정)으로 정리한다
- Markov Decision Process(MDP)란 강화학습 문제를 기술하는 수학적 표현 방법
- MRP(마르코프 보상 과정)에 **행동을 추가**한 확률 과정

- 선재적으로 확률 과정 이해 필요
  - 마르코프 과정(Markov Processes) = 마르코프 연쇄(Markov Chain)
  - 마르코프 보상 과정(Markov Reward Processes : MRP)
  - 마르코프 결정 과정(Markov Decision Process : MDP)

### [Markov property]
- 현재 상태(S_{t})를 알면 역사를 아는 것과 동일한 수준으로 미래 상태를 추론할 수 있다.
- 현재 상태를 아는 것이 과거의 전부를 아는 것과 동일하다.
- 미래의 상태는 과거와 무관하게 현재의 상태만으로 결정된다.
- P(S_{t+1}|S_{t}) = P(S_{t+1} | S{t}, S{t-1}, ..., S{0})

    
</br>

### [Markov Processes Markov Chain]
- 마르코프 과정(Markov Process) 또는 마르코프 연쇄(Markov Chain)로 불림 </br>
- 상태 변화에만 관심이 있음 </br>
- <S,P> 인 튜플 </br>
  - S : 유한한 모든 상태의 집합 </br>
  - P : 상태 천이 행렬 (State Transition Matrix) : 확률을 행렬 내에 표현한 것 (가로 합 : 1, 각 원소 >=0) </br>
    - P_{ss'} : 현재 상태s에서 다음 상태 s'으로 이동할 확률 </br>
      - P_{ss'} = P(S_{t+1} = s'| S_{t} = s) </br></br>

    ```
    S = {서울, 대전, 원주, 광주, 대구, 울산, 부산}
    ```
    - 현재 '서울' 상태일 때 </br>
    - 다음 시점 원주, 대전, 광주로 이동할 확률이 1/4, 1/2, 1/4이면 </br>
    - 상태 천이 행렬(P)은 각 확률을 행렬 내에 표현한 것 </br>
    ```
    P(S_{t+1} = 원주 | S_{t} = 서울) = 1/4
    P(S_{t+1} = 대전 | S_{t} = 서울) = 1/2
    P(S_{t+1} = 광주 | S_{t} = 서울) = 1/4 
    ```
- 활용
  - 그래프 형식으로 표현되면 상태 천이 행렬(P)로 표시 가능 </br>
  - 가로 합은 항상 1 </br>
  - 종결 상태는 항상 자기 자신으로 돌아옴 </br>
    - 상태 (원) </br>
    - 상태 천이 방향, 확률 (선) </br>
    - 자가 연결(자기 연결) : 한 시점 뒤 자기자신으로 돌아옴 </br>
    - 종결 상태(terminal state) : 들어오면 더 이상 움직이지 않는다 </br>
      - 종결 상태가 정의되어 있기 때문에 적당한 조건 내에서 끝나게 됨 </br>
      - **Episode** : 임의의 상태에서 도달하는 순간까지의 상태들 </br>
        -> 가치 기반 강화 학습 일종 **'몬테카를로 기법'**에서 주요 개념 </br></br>
        ```
        Ep1. 서울 -> 대전 -> 대구 -> 부산
        Ep2. 서울 -> 대전 -> 대구 -> 울산 -> 부산
        Ep3. 서울 -> 원주 -> 원주 -> 대구 -> 울산 -> 부산
        Ep4. 서울 -> 광주 -> 부산
        ```
    ---
    ### 목차
    
    **1. 강화학습 개요** </br>
      - [강화학습의 본질](#강화학습의-본질) </br>
      - [강화학습의 특징](#강화학습의-특징) </br>
      - [강화학습의 구성요소](#강화-학습의-구성-요소) </br>
    
    **2. 고전적 강화학습** </br>
      - [고전적 강화학습 분류](#고전적-강화학습-분류) </br>
      - [마르코프 의사 결정](#마르코프-결정-과정) </br>
          - [Markov Processes Markov Chain](#markov-processes-markov-chain) </br>
          - [마르코프 보상 과정](#마르코프-보상-과정) </br>
            - [리턴](#리턴) </br>
            - [가치 함수(Value Function)](#가치-함수value-function)
      - [강화학습 문제 풀이 기법](#강화학습-문제-풀이-기법) </br>
    ---

### [마르코프 보상 과정]
- **MRP**
- 한 번의 상태 천이가 보상 또는 패널티를 가지는 경우가 현실적으로 많다 (diff. 마르코프 연쇄)
- 마르코프 과정에 '보상'을 추가한 확률 과정
- <S, P, R, γ> 인 튜플
  - S : 유한한 상태의 집합
  - P : 상태 천이 행렬
  - **R** : 보상 함수 (R : S -> R)  # 확률적 / 결정적일 수 있음 (스칼라)
  - **γ** : 감소율, γ ∊ [0,1]       # 0 이상 1 이하의 실수
    - 일반적으로 1보다 작은 값 </br>
      - 무한히 커지는 것 방지. 수치적으로 안정적 </br>
      - 수학적 분석이 용이해짐 </br>
      - 먼 미래는 불확실하다는 철학 반영 </br>
      - 사람은 먼 미래에 실현되는 이익을 선호하지 않음 </br>
      ! 에피소드가 끝나는 경우 γ=1.0을 사용하기도 함 </br>
    

#### [리턴]
- 강화학습 agent가 현재 상태로부터 미래에 일을 고려할 수 있게 해주는 장치
- 리턴(G_{t}) : 현재 시점부터 전체 미래에 대한 "**감가**된 **보상**의 합" </br>
- 보상의 감가 총합 </br>
  - 확률 변수 </br>
  - γ값(감소율)
    - 1에 가까워질수록 :
      - 미래 보상의 합이 return에 미치는 영향이 커지게 됨
      - 미래 고려 커짐
    - 0에 가까워질수록 :
      - 미래에 대한 고려x
  - R_{t}
    - 확률변수로 존재
    - 하나의 결정된 값이 아님.

#### [MRP의 가치 함수(Value Function)]
- 가치함수 V(s) : 현재 상테에서 미래의 리턴(감가된 보상의 합)의 기댓값
- 리턴은 확률 변수로 대수 비교나 연산이 번거로움
- 리턴의 평균을 취해 확률변수에서 하나의 정의된 **스칼라 값**으로 바꿔준다.
- 직관적으로 가치함수를 다룰 수 있게 됨

#### [Bellman 방정식] (BEE)
- 가치 함수를 정의하면서 항등식을 정의할 수 있게 됨
- 계산적으로 가치함수를 추정하는데 활용됨
- 선형적(선형 방정식) -> 일반해를 얻을 수 있다.

### [마르코프 결정 과정 MDP의 태동]
- 흘러가는대로 상태와 보상을 받아야 했다
- 개념을 추가해 **보상이 유리하게 하는 장치 마련이 필요**했다.
- 행동을 추가하는 MDP(마르코프 결정 과정)이 등장


### [마르코프 결정 과정]
- **MDP(마르코프 결정 과정)** : 마르코프 보상 과정(MRP)에 **행동을 추가**한 확률 과정
- 최적 가치함수 또는 최적 정책을 얻는 것
  > 모델이 주어진 경우, 최적 가치 함수와 최적 정책은 상호 보완적
- contribution : 환경에 **행동**을 가하면서 **미래와 상태, 보상**을 바꿀 수 있게 됨
- 적합한 행동을 가해 원하는대로 흘러가도록 할 수 있음
- <S, A, P, R, γ> 인 튜플
  - S : 유한한 상태의 집합
  - **A** : 유한한 행동의 집합 (Action)
  - P : 상태 천이 행렬
    - 상태 천이 행렬이 변함 (A를 가해 다음 상태 행렬 바꿀 수 있게 되었음)
    - 더 이상 매트릭스로 표현하지 못하고 **3D 구조로 표현**해야 함
    - P_{ss`}^α = P[S_{t+1} = S' | S_{t} = s, A_{t} = α]
  - R : 보상 함수 (R: S * A -> R) # 확률적/ 결정적일 수 있음
    - 보상 함수 역시 현재 상태, Action에 따라 영향 받음
  - γ : 감소율, γ ∊ [0,1] # 0 이상 1 이하 모든 실수 중 하나

### [정책함수(Policy Function)]
- 정책함수(π) : **현재 상태**에서 수행 할 **행동의 확률 분포**
- π(α|s) = P(A_{t}=a | S_{t} =s)
- 강화학습 agent는 **현재 상태 s_{t}**를 활용해 **현재의 행동 α_{t}를 결정**한다. (agent : 상태->행동)
- Markov 특성을 가정하였기 때문에 현재 상태만 가지고 의사결정을 해도 충분하다
  - 현재 상태를 아는 것이 역사를 아는 것과 동일하다

### [MDP, MRP, MP의 관계]
- **MDP(마르코프 결정 과정)과** **정책(π)이 결정**되었을 때 MDP를 통해 얻은 **상태들은 마르코프 과정**을 따름
- **상태**들과 **보상**들은 **마르코프 보상 과정**을 따르게 됨.
- 좋은 정책을 갖고 있다면, 최대한 많은 이득을 얻는 것이 가능하다
  - 마르코프 보상 과정의 상태 천이 행렬과 보상 함수는 현재 정책 π에 영향을 받게 됨
  - 정책 π가 시스템의 성질을 바꾼다
- 좋은 정책이란? 무엇이 최적의 정책을 결정?
  - 최적 정책 정의를 위해 '최적 가치 함수(Optimal Value Function)'이 정의되어야함
    - 최적 상태 가치 함수 : 존재하는 모든 정책(π)을 모두 고려했을 때 가장 높은 가치 함수를 주는 정책의 가치함수값
    - 최적 행동 가치 함수도 정의되게 됨.
    - **최적 가치 함수** 또는 최적 가치함수를 만드는 **최적 정책**을 찾았을 때 **MDP를 풀었다**고 표현.
    - 최적 정책들 : 최적 정책이 유일하게 존재하는 것은 아님.
  

#### [MDP(마르코프 결정 과정)의 가치함수]
- MRP에 없는 **행동**에 대한 개념이 추가되어 새로운 가치 함수가 정의됨
- **상태 가치 함수(State value funtion)**
  - V_{π}(s) = E_{π}[G_{t}|S_{t}=s]  # MRP의 가치함수와 매우 유사하게 생김
  - 현재 t 상태 s에서 정책 π을 따른다면 얻을 미래 가치의 감가 총합
    - **정책이 결정되면 MDP -> MRP 바뀜**
    - 같은 MDP여도 어떤 정책을 따르느냐에 따라 가치 함수의 값이 달라지게 됨 -> 아래첨자 활용(π)
- **행동 가치 함수(State-action value function)**
  - MDP는 action이 명확히 정의됨에 따라 행동 가치 함수를 정의
  - 현재 t 상태 s에서 a라는 행동을 취한 후 정책을 따른다면 얻을 미래의 가치의 감가 총합
  - Q_{π}(s) = E_{π}[G_{t}| S_{t}=s, A_{t}=a] # Q-learning의 Q임
- **상태 가치 함수 V와 행동 가치함수 Q의 관계**
  - 행동 가치 함수 -> 상태 가치 함수 :
    - 상태 가치 함수 = SUM(행동 가치 함수 * 각 행동의 확률)
  - 상태 가치 함수 -> 행동 가치 함수 :
    - 현재 상태에서 a라는 행동 -> R_{s}^a라는 보상을 받게 되고, 어떤 s'에 도달하게 됨
    - 도달한 각 s'에서 각각의 가치는 V_{π}(s')
    - s'에 대한 확률 P_{ss'}^a와 V_{π}(s')를 활용해 평균 낸 값 -> γ만큼 감가
  - 현재 상태 가치는 2가지로 분리됨
    - 현재 정책을 통해 평균적으로 얻을 수 있는 보상
    - 현재 정책을 통해 다음 상태(s')로 전이하게 도면 평균적으로 얼마만큼의 가치함수로 얻을 수 있는지

#### [Bellman 최적 방정식 BOE]
- 벨만 기댓값 방정식의 π에 최적 정책 π* 대입해 얻음
- BOE는 선형 방정식이 아니기 때문에 일반해가 존재하지 않음
- 반복적 알고리즘을 통해 해를 계산하게 됨
  - policy evaluation / Policy iteration
  - Value iteration
  - Q-learning
  - SARSA
  - ...


# [강화학습 문제 풀이 기법]
강화학습 문제를 MDP로 정의했고, MDP를 푼다는 것은 최적 정책을 얻는 것이다. </br>
최적 정책은 벨만 최적 방정식 BOE를 통해 구할 수 있다. </br>
BOE를 구하는 방식은 환경에 대한 정보 유무에 따라 나눌 수 있다. </br>

일반적으로 환경에 대해 알고 있으면 더 해결이 쉽고, 효율적으로 문제 해결이 가능하다. </br>
환경에 대해 모든 정보를 안다는 것은 현실적이지 않다. </br>
환경에 대해 모르는 경우 환경과의 상호작용을 통해 문제를 해결하는 방법을 배워야 한다. </br>
학습을 통해 환경을 이해하기 때문에 현실의 문제 상황에 적용 가능하다. </br></br>
  
- 다양한 풀이 기법 존재하나, 강화 학습 문제 구조 알고 있을 때 동적 계획법 주로 다룰 예정
- 환경에 대해 알 때의 마르코프 의사결정 풀이 기법 : DP
- 환경에 대해 모를 때 마르코프 의사결정 풀이 기법 : MC, TD

  |구성|환경에 대해 알 때| 환경에 대해 모를 때|
  |----|:--------------:|:------------------:|
  |대표|DP (동적 최적화, Dynamic Programming)| MC, TD|
  |장점|(상대적) 문제 해결 용이|(DP 대비) 효율성 떨어짐|
  |장점|매우 효율적임     | 현실의 문제 상황에 적용 가능|
  |단점|현실적이지 않음|                       |

</br>

- 동적 계획법 알고리즘 </br>
  : 환경에 대한 정보를 모를 때 학습을 통해 마르코프 의사 결정 과정 문제 푸는 이론적 근간이 됨 </br></br>
  - 정책 평가
  - 정책 개선
  - 가치 방법 

## [동적 계획법 Dynamic Programming]
- 복잡한(큰) 문제를 작은 문제로 나눈 후 작은 문제의 해법을 조합해 큰 문제의 해답을 구하는 기법을 총칭 </br>
- 작은 문제의 해법을 조합하는 방식은 대부분 반복적인 방식을 통해 이뤄짐 </br>
- MDP에서 정의한 Bellman 기대/최적 방정식은 DP로 해결하는 문제 2가지 특성을 만족함 </br>
- DP를 이용해 Bellman 기대/최적 방정식 해를 계산 할 수 있게 됨 </br></br>
  
- 동적 계획법으로 해결할 수 있는 문제 2가지 </br>
  - 최적 하위 구조 (Optimal substructure) </br>
    - 큰 문제를 분할한 작은 문제의 최적값이 큰 문제에서도 최적값 </br>
    - Principle of optimality라고 불림 </br>
  - 중복 하위 문제 (Overlapping Problems) </br>
    - 큰 문제의 해를 구하기 위해 작은 문제의 최적 해를 재사용 </br>
    - 여러 번 재사용하기 때문에 일반적으로 테일블에 저장해둠 </br>

  ### [DP를 이용해 벨만 기대 방정식 풀기]
  - 벨만 기댓값 방정식은 직접해를 구할 수 있었지만 역행렬 연산이 불가한 경우가 있어 새로운 알고리즘을 도입
    - MDP의 상태 또는 행동 공간이 커졌을 때 (상태와 행동의 종류가 많아졌을 때)

  - **정책 평가 알고리즘 (Policy evaluation : PE)**
    - 정책 평가 : 반복적인 과정을 통해 Bellman 기댓값 방정식 해를 구하는 방식 중 하나 </br>
    - 평가하고 싶은 정책 π가 있을 때 정책의 가치함수를 임의의 값으로 초기화 </br>
      - 어떤 값을 넣어도 알고리즘의 값은 하나의 값으로 수렴하게 됨 </br>
      - 벨만 기댓값 연산을 변형한 형태의 연산을 반복적으로 수행 </br>
      - 반복 이전과 반복 이후의 가치 함수의 차이인 V_{t+1}^π(s) - V_{t}^π를 계산하게 됨 </br>
      - 차이는 벡터값으로, 벡터 값 중 가장 큰 값이 차이가 아주 작은 양수 ∊보다 작아졌는지 계산하게 됨 </br>
      - 작게 되면 한 번 더 동일한 과정을 반복하게 됨 </br>

- **정책 개선 (Policy Imrpovement : PI)**
  - 벨만 최적 방정식의 해를 구하는 방법 </br>
  - 현재 정책 π로부터 개선된 정책 π'을 구하는 알고리즘 </br>
  - 특정 상황에서 가장 높은 행동 가치를 보이는 행동만 1의 확률로, 나머지는 수행x </br>
  - 새로운 정책이 좋은지? (π > π') </br>
    - **정책 개선 정리 (Policy Improvement Theorem)** </br>

**- 정책 반복 (Policy Iteration)**
  - 정책 평가와 정책 개선을 적용해 Bellman 방정식을 푸는 알고리즘
  - 임의의 정책으로부터 개선된 정책을 만들어내는 알고리즘
  - 정책 반복 알고리즘을 계속적으로 반복해 임의의 정책으로부터 최적 정책을 계산해낼 수 있음

