# Reinforcement Learning

### 목차

**1. 강화학습 개요** </br>
  ㄴ [강화학습의 본질](#강화학습의-본질) </br>
  ㄴ [강화학습의 특징](#강화학습의-특징) </br>
  ㄴ [강화학습의 구성요소](#강화-학습의-구성-요소) </br>

**2. 고전적 강화학습** </br>
  ㄴ [고전적 강화학습 분류](#고전적-강화학습-분류) </br>

---

### [강화학습의 본질]
- 강화학습이 풀고 싶어하는 문제 : 주어진 상황에서 가장 좋은 행동을 하는 것 </br></br>


### [강화학습의 특징]
**1. 정답이 아닌 '보상'이 주어짐**
   - label, 정답이 주어지지 않음
   - 보상이 주어짐 (좋고, 나쁨을 평가)

**2. 현재 의사 결정이 미래의 의사결정에 영향을 미침**</br></br>
**3. 에이전트는 문제의 구조를 알 수 없음**
   - 환경과의 상호 작용을 통해 정보를 관측
   - 관측된 정보를 최대한 모아서 학습하게 됨 </br></br>


### [강화 학습의 구성 요소]
- **agent, 환경**
  - **agent** : '학습의 주체'로 환경과의 상호작용을 함
  - **환경** : 강화 학습 문제의 특성을 지님 </br></br>

- **일반적인 상호작용의 과정**
  - 매 시점마다 agent는 환경으로부터 현재를 관측 : s_{t}
  - 관측된 내용을 통해 적합한 행동을 환경에 가함  : a_{t}
  - agent의 행동(a_{t})에 **영향을 받은 환경은 agent에** 다음을 전달</br>
</br></br>

**[환경과 agent의 구성요소]**
- **1. 환경** : agent에 보상(reward)와 상태(state)를 전달
  - **다음 상태** : s_{t+1}
  - **보상** : r_{r+1} </br></br>
    - **보상** : 하나의 숫자로 표현된 행동에 대한 평가지표
      - 현재 상태에 agent에 가해진 action이 얼마나 좋은지, 나쁜지에 대한 평가지표
      - 예시) 길찾기 문제에서 각 도시 사이의 거리 </br></br>
    - **상태** : 문제의 현재 상황을 잘 기술하는 정보
      - 예시) 길 찾기 문제에서 에이전트의 현 위치 </br></br>

- **2. agent의 구성요소**
  - 정책, 가치함수, 모델
  - **정책(policy)**
    - RL의 목적이 "좋은 정책을 찾는 것"과 밀접하므로 가장 중요
    - 특정 상황에서 agent가 어떻게 행동을 할지 결정해주는 함수
  - **가치 함수(value function)**
    - 환경이 agent에 보상을 전달해주지만, agent는 환경에 대한 선험적 지식이 없음
    - 가치함수를 통해 현재 상태 및 행동이 얼마나 좋은지 추산
  - **모델**
    - agent가 추측하는 세상의 형태
    - agent 내부에 존재하는 환경에 대한 추측치

---
### 목차
**1. 강화학습 개요** </br>
  ㄴ [강화학습의 본질](#강화학습의-본질) </br>
  ㄴ [강화학습의 특징](#강화학습의-특징) </br>
  ㄴ [강화학습의 구성요소](#강화-학습의-구성-요소) </br>

**2. 고전적 강화학습** </br>
  ㄴ [고전적 강화학습 분류](#고전적-강화학습-분류) </br>

---

### [고전적 강화학습 분류]
- 환경에 대한 모델이 있는지 없는지에 따라 분류된다.</br>
  - 모델 기반 강화학습 </br>
  - 모델 없는 강화학습 </br>
    - 일반적으로 지칭되는 '강화학습' </br>
      (1) 가치 기반 강화학습 : 가치 함수를 직접적으로 추산하는 방법 </br>
      (2) 정책 기반 강화학습 : 가치 함수 대신 바로 정책 함수를 추산 </br>
      (3) Actor-Critic 알고리즘 </br>
        - 가치 기반 강화학습, 정책 기반 강화학습의 장점을 살린 새로운 계열 알고리즘 </br>
