{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNqQUsAGM7lNPFGuN6vIpQ6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MinsooKwak/Study/blob/main/LLM/BERT/BERT_mask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGB8sU6vU8dF",
        "outputId": "0b682a9e-decf-4427-fddb-02cd13ceb100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.33.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification"
      ],
      "metadata": {
        "id": "0xngwsjHVLPs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load modal and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1QBfgu3VSgy",
        "outputId": "caae3331-5c57-4db2-ca32-05eec8587316"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "7SduOuUlVfey"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text classification function\n",
        "def classify_text(text):\n",
        "  inputs = tokenizer.encode_plus(\n",
        "      text,\n",
        "      add_special_tokens=True,\n",
        "      padding='max_length',\n",
        "      truncation=True,\n",
        "      max_length=128,\n",
        "      return_tensors='pt'\n",
        "  )\n",
        "  input_ids = inputs['input_ids'].to(device)\n",
        "  attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "  # Forward pass through model\n",
        "  with torch.no_grad():\n",
        "    outputs = model.to('cuda')(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "  logits= outputs.logits\n",
        "  probabilities = torch.softmax(logits, dim=1).squeeze(dim=0)\n",
        "  predicted_class = torch.argmax(probabilities).item()\n",
        "\n",
        "  return predicted_class, probabilities"
      ],
      "metadata": {
        "id": "AhMsf5U5Vo0F"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "text_to_classify = \"This is an example sentence\"\n",
        "predicted_class, probablities = classify_text(text_to_classify)"
      ],
      "metadata": {
        "id": "z4NxFpYZWxfG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Predictec class : {predicted_class}')\n",
        "print('Probabilities :')\n",
        "for i, prob in enumerate(probablities):\n",
        "  print(f'Class {i} : {prob.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edlQr4QFXWgM",
        "outputId": "b280bcc0-d305-4a73-cf65-2ee8bc4e2014"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictec class : 1\n",
            "Probabilities :\n",
            "Class 0 : 0.34375154972076416\n",
            "Class 1 : 0.6562485098838806\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hm1Yqk1nYez9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM"
      ],
      "metadata": {
        "id": "WrYdfJkwYmrc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForMaskedLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgeiCsm2Yq7e",
        "outputId": "f3c189ba-3dae-4143-8da5-215cfc6120c9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(text):\n",
        "  # Tokenize input Text\n",
        "  tokenized_text = tokenizer.tokenize(text)\n",
        "  masked_index = tokenized_text.index('[MASK]')\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "  # Convert tokens to tensor\n",
        "  tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n",
        "\n",
        "  # Forward pass\n",
        "  with torch.no_grad():\n",
        "    outputs = model.to('cuda')(tokens_tensor)\n",
        "\n",
        "  predictions = outputs[0][0, masked_index].topk(k=5).indices.tolist()\n",
        "\n",
        "  predicted_tokens=[]\n",
        "  for token_index in predictions:\n",
        "    predicted_token = tokenizer.convert_ids_to_tokens([token_index])[0]\n",
        "    predicted_tokens.append(predicted_token)\n",
        "\n",
        "  return predicted_tokens"
      ],
      "metadata": {
        "id": "FRMhn3qcY2a7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#example\n",
        "text_with_mask = 'I want to [MASK] a pizza for dinner'\n",
        "predicted_tokens = predict_next_word(text_with_mask)\n",
        "\n",
        "print(predicted_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RZQ9RFcZ4d5",
        "outputId": "f4f20601-c609-4c71-b5f8-a0dfb5a176f1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['have', 'get', 'be', 'make', 'take']\n"
          ]
        }
      ]
    }
  ]
}