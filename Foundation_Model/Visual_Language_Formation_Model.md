## Visual Language Formation Model
- VOLTA : Vision and Language Transformers 논문에서 처음 제시 (2021)
- 이후 주요 논문
  - VILLA : Visual Question Answering in Low Resources Languages(2021)
  - CLIP (2021)
  - DALL-E (2021)
  - ImageBERT (2022)


### 관련 모델

- **Image Caption**
  - 이미지에 대한 설명 생성
  - 시각 정보를 분석하고 텍스트로 설명
  - 예시 ) Show and Tell, Oscar, CLIP, ConVLAD

- **Visual Question Answering(VQA)**
  - 이미지와 과련된 질문에 답변을 생성하거나 선택
  - 예시 ) VQA, MRB and MUTAN, DALL-E, CLIP, VL-BERT
 
- **Image Retrieval (이미지 검색)**
  - 텍스트 쿼리에 대한 이미지 검색 수행
  - 주어진 텍스트로 가장 관련된 이미지 찾음
  - 예시 ) SCAN(Structure-Content Aware Network), COCO(Contextualized Object-Categor Retrieval), CLIP, SwAV, MoCo

- **Visual Dialog**
  - 이미지 기반 대화 생성하거나 이해
  - 이미지와 텍스트 간의 대화 스레드를 모델링
  - 예시 ) VisDial, DialogGPT

- **Text-to-Image Generation**
  - 텍스트 설명 기반 이미지 생성
  - 예시 ) AttnGAN, CLIP, BigGAN

- **Visual Storytelling**
  - 시퀀스 형태 이미지 텍스트 결합해 이야기 생성하거나 이해
  - 예시 ) StoryGAN, Storytelling Transformer

- **Visual Reasoning (시각적 추론)**
  - 이미지와 텍스트 사용해 추론 및 문제해결 수행
  - 이미지와 텍스트간의 관계를 분석함
  - 예시 ) Clevrer, GOA(Visual Question Answering with a Focus on Reasoning), VQA-HAT, GQA

- **Multimodal Translation**
  - 이미지와 텍스트간의 상호 번역 수행
  - 이미지 설명을 다른 언어로 번역하거나, 텍스트를 이미지로 번역
  - 예시 ) mBERT, XLM-R, CLIP
 
- **Visual Commonsense Reasoning (시각적 상식 추론)**
  - 이미지와 텍스트 기반으로 상식적인 추론을 수행
  - 일상적 상황을 이해함
  - 예시 ) VCR(Visual Commonsense Reasoning), VL-BERT
 
- **Visual Sentiment Analysis (시각적 감정 분석)**
  - 예시 ) UniViLM, CLIP
  
