# Study

### 0. Multimodal
- [MultiModal Intro : readme](https://github.com/MinsooKwak/Study/blob/main/MultiModal/README.md)

- Multi-Modal Models
  - XLM-PropheNet
  - BiTransformer
  - DPR
  - CodeBERT
  - Stable Diffusion V2
  - ViT-GPT2
  - ViLT finetuned on VQAv2
  - LayoutLM
  


### 1. NLP
- LLM model
  - AutoRegressive Model
    - GPT2 (transformer의 decoder 쌓아 만들었음)
    - Transformer XL (recurrent decoder 쌓아 만들었음)
    - Reformer (시간 복잡도 개선) : O(L^2) -> O(LlogL)
      
  - Encoder Model :
    - BERT (transformer의 encoder 쌓아 만들었음)
    - ALBERT
    - RoBERTa
    - DistilBERT
    - ConvBERT
    - XLM-RoBERTa
    - FlauBERT
    - ELECTRA
    - Longformer
  
  - Seq2Seq Models
    - BART
    - PEGASUS
    - MT5

- sLLM </br></br>


### 2. CV
- GLPN
- Vit_Large
- DETR with ResNet backbone
- CLIPSeg
- Stage Diffusion
- Denoising
- VideoMAE base-sized model </br></br>


### 3. Audio
- Cloning_BARK
- SpeechBrain </br></br>


### 4. Flutter
  - Google에서 개발한 크로스 플랫폼 개발 Framework
  - Multi-size Layout display
  - Material Design
  - Flatform / application
    - Mobile Flatform : iOS, Android
    - Desktop / Web Application : Windows, MacOS, Linux 등...
